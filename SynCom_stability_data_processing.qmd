---
title: "SynCom co-culture experiments"
author: "Marine C. Cambon"
date: last-modified
published-title: "document last compilation"
format:
  html:
    theme: 
      light: minty
      dark: slate
    linestretch: 1.7
    toc: true
    toc-location: left
    code-fold: true
    code-summary: "Show the code"
    self-contained: true
    link-external-newwindow: true
    # comments:
    #   hypothesis: true
editor: source
#bibliography: documentation/references.bib
editor_options: 
  chunk_output_type: console
---

```{r setup, include = FALSE}
rm(list=ls())
library(targets)
```

## Setting up renv, {targets} and packages

```{r, eval=FALSE}
renv::install("bioc::dada2")
renv::install("targets")
renv::install("tarchetypes")
renv::install("quarto")
renv::init()

targets::use_targets()
```

```{r}
tar_visnetwork()
```


## Read demultiplexing

The run contains multiplexed samples and only one primer pair. The full sequences of primers + spacer + tag are in `data/tag_primers_16SV4_fwd.fasta` and `data/tag_primers_16SV4_rev.fasta`.

The raw data were downloaded from Novogene on BlueBEAR at `~/cambonm-oak-syncoms/250730_SynCom_stability_experiment/01_raw_data`

MD5sum of the file is checked and the file is then uncompressed

```{.bash}
md5sum -c MD5.txt
> X204SC24085297-Z01-F002.tar: OK

tar -xvf X204SC24085297-Z01-F002.tar
cd X204SC24085297-Z01-F002/01.RawData/pool_1/
md5sum -c MD5.txt 
> pool_1_FKDN250298699-1A_22NW5FLT4_L5_1.fq.gz: OK
> pool_1_FKDN250298699-1A_22NW5FLT4_L5_2.fq.gz: OK
```


Because I sent a pool of sequences to Novogene which then get ligated to Illumina adapters, fragments are inserted in random orientation and need to be demultiplexed both ways and with both primer and tag!

:::{.callout-tip}
In later versions of cutadapt, there is the `--revcomp` option to automatically look for primers in both orientation.

I tested the option and my old loop based version and seemed to get more or les the same unknown-unknown files size so I'll just go ahead with the `--revcomp` option from now on.
:::


### Running `cutadapt`

I ran [Cutadapt](https://cutadapt.readthedocs.io/en/stable/) v4.9 on blueBEAR:

<div style="height:300px;overflow:auto;">
```{.bash}
#!/bin/bash
#SBATCH -o cutadapt.revcomp.demultiplex.o.%J    # Job output file
#SBATCH -e cutadapt.revcomp.demultiplex.e.%J    # Job error file
#SBATCH --ntasks=4            # number of parallel processes (tasks)
#SBATCH --qos=bbdefault       # selected queue
#SBATCH --time=72:00:00        # time limit

set -e

module purge
module load bear-apps/2023a
module load cutadapt/4.9-GCCcore-12.3.0

## 16SV4
export input_dir=/rds/projects/c/cambonm-oak-syncoms/250730_SynCom_stability_experiment/01_raw_data/PE250/X204SC24085297-Z01-F003/01.RawData/pool_1/
export output_dir=16SV4_revcomp
mkdir $output_dir

# --cores=0 auto detects the number of available cores
# --revcomp searchs for primers and adapters in both orientations

cutadapt \
    -e 0.15 --no-indels \
    --cores=0 \
    --revcomp \
    -g file:tag_primers_16SV4_fwd.fasta \
    -G file:tag_primers_16SV4_rev.fasta \
    -o $output_dir/{name1}-{name2}.R1.fastq.gz -p $output_dir/{name1}-{name2}.R2.fastq.gz \
    $input_dir/pool_1_1.fq.gz $input_dir/pool_1_2.fq.gz
```
</div>

### Read number per samples after demutliplexing

I counted the number of read per sample after demultiplexing with the following code:

<div style="height:300px;overflow:auto;">
```{.bash}
#!/bin/bash
#SBATCH -o count.demultiplex.o.%J    # Job output file
#SBATCH -e count.demultiplex.e.%J    # Job error file
#SBATCH --ntasks=1            # number of parallel processes (tasks)
#SBATCH --qos=bbdefault       # selected queue
#SBATCH --time=00:05:00        # time limit

set -e

module purge
module load bear-apps/2023a/live

touch count_demultiplexed.txt

for f in 16SV4_revcomp/*R1.fastq.gz
do
    c=$(zcat $f | grep @ | wc -l)
    echo $f $c >> count_demultiplexed.txt
done
```
</div>

Then plot the results


```{r}
tar_load(fig_read_counts_demultiplex)
fig_read_counts_demultiplex
```

Every single sample has more than 1000 reads including the negative controls and unused tags! Because there were quite few samples in the pool the sequencing depth is very high. I will need to be quite stringent in the cleaning steps.


## Read quatlity filtering 

### Defining paths

Output path:


### Quality filtering

Checking read quality profiles for a random sample:


### Checking and filtering sequence lenght
 
```{r}
tar_load(fig_seqlength)
fig_seqlength
```
 
Wow, all ASVs are pretty much the same size, I don't think I need to filter more on the length ! I'll do it anyway just to build the target for future use.

```{r}
tar_load(seqtab_filt)
seuil <- nchar(rownames(seqtab_filt))
```

After filtering, all sequences are `r min(seuil)` and `r max(seuil)` bp and the dataset has `r nrow(seqtab_filt)` ASVs.

### Removing chimera

```{r}
rm(seqtab_filt)
tar_load(seqtab_filt_nochim)
```

After removing chimera, the dataset has `r nrow(seqtab_filt_nochim)` ASVs.

### Read loss


```{r}
tar_load(fig_read_loss)
fig_read_loss
```

